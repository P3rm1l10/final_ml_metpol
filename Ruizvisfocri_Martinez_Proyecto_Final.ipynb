{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d1fc53-d94b-4499-817d-babbcb01e6b1",
   "metadata": {},
   "source": [
    "# Proyecto final Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6795c-ca8c-440d-bfeb-9b7607685711",
   "metadata": {},
   "source": [
    "Jorge Ruizvisfocri\n",
    "\n",
    "Emilio Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a7f13-99cc-46fe-a2cb-7f7bafe43117",
   "metadata": {},
   "source": [
    "# Descripción del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d469410-9a0e-4207-9278-5fe08489dffe",
   "metadata": {},
   "source": [
    "El siguiente proyecto busca clasificar imágenes de Carcinomas Ductales Invasivos (IDC en inglés) extraidas de muestras de pacientes con cancer de mama.\n",
    "\n",
    "La base de datos fue tomada de https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images\n",
    "\n",
    "Se propone utilizar una red neuronal convolucional para resolver el problema de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61434f9f-74a5-4dc9-ad23-b654c5949587",
   "metadata": {},
   "source": [
    "# Paqueterías de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c300dce-540b-42bd-a5b5-b8a14977d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paquetes de ciencias de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "import random\n",
    "\n",
    "## Paquetes de lectura de imagenes\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "## Paquetes de imágenes\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "##Otros\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ff47-7aa4-469d-9694-113bfeac7d62",
   "metadata": {},
   "source": [
    "# Datos de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74b69f5-85ae-4e92-8d98-1da5c635cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio de imágenes\n",
    "data_dir = Path(\"D://bases de datos//proyecto_ml_final//imagenes\") ## Directorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1eb82a-b910-47a6-a3d1-fcc431b716e2",
   "metadata": {},
   "source": [
    "Dado que todos los pacientes tienen muestras con tumores malignos y benignos, la aleatorización puede realizarse a nivel de imagen o a nivel de paciente.\n",
    "\n",
    "Bajo el supuesto de que podría existir correlación de algún tipo entre las imágenes pertenecientes a un mismo paciente, creemos que sería interesante aleatorizar a nivel de paciente, para introducirle a la red la información de personas que jamás ha visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ba3705-8755-4f5a-a53d-dfc487503281",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aleatorización a nivel de paciente\n",
    "#### Obtenemos la lista de pacientes\n",
    "px = [f for f in data_dir.iterdir() if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c287e8-2951-4e57-afce-25b89086617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtenemos el número equivalente al porcentaje deseado\n",
    "k = math.ceil( len(px) * 80 // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14f1fd-217c-4779-a803-1bef7cc3b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hacemos la muestra de pacientes para entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d71ef82-8a3e-47b5-a8b3-6b45e7bda23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fijamos la semilla para tener reproductibilidad\n",
    "random.seed(4352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8cba47-2ad2-476a-b6d4-ed92d1e40714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = random.sample(px,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f427d55e-c414-4879-8be7-0176db51250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = set(px) - set(train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e5ae93-7109-43b4-b1e3-414fc2a62555",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Limpieza\n",
    "del data_dir\n",
    "del px\n",
    "del k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c6dd66-db7a-4b6d-9677-8c4f98bf6603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 223/223 [00:32<00:00,  6.80it/s]\n"
     ]
    }
   ],
   "source": [
    "### cargamos files de entrenamiento\n",
    "train_images_files = []\n",
    "for p in tqdm(train_id):\n",
    "    img_lst = list(p.rglob(\"*.png\"))\n",
    "    train_images_files.extend(img_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54b8fb0-5222-4e3a-85f4-b6629b21cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [00:09<00:00,  6.10it/s]\n"
     ]
    }
   ],
   "source": [
    "test_images_files = []\n",
    "for p in tqdm(test_id):\n",
    "    img_lst = list(p.rglob(\"*.png\"))\n",
    "    test_images_files.extend(img_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2694b47-372e-4572-a031-7ce7f90cc5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imagenes\n",
      "-----------------\n",
      "Total: 277524\n",
      "-----------------\n",
      "Entrenamiento: 220389\n",
      "-----------------\n",
      "Prueba: 57135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Revisamos que las direcciones de las imágenes estén bien cargadas\n",
    "print(\n",
    "    f\"Número de imagenes\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Total: {len(train_images_files) + len(test_images_files) }\\n\"  # 277,524 tiles\n",
    "    \"-----------------\\n\"\n",
    "    f\"Entrenamiento: {len(train_images_files) }\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Prueba: {len(test_images_files) }\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974df814-489d-45e7-8008-511351b9273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lipieza\n",
    "del train_id\n",
    "del test_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984508a-0785-4ac4-bdfb-32a840c306f9",
   "metadata": {},
   "source": [
    "## Preparamos las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0131a0b1-a854-49d1-9fe2-0327ddb64e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 220389/220389 [00:00<00:00, 223263.86it/s]\n"
     ]
    }
   ],
   "source": [
    "## y_entrenamiento\n",
    "### Preparamos las etiquetas\n",
    "y_train = []\n",
    "for name in tqdm(train_images_files):\n",
    "    etiqueta = str(name)[-5]\n",
    "    etiqueta_num = int(etiqueta)\n",
    "    y_train.append(etiqueta_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62fb3993-3c47-479d-afe9-18b5f9fafd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 57135/57135 [00:00<00:00, 200750.89it/s]\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "for name in tqdm(test_images_files):\n",
    "    etiqueta = str(name)[-5]\n",
    "    etiqueta_num = int(etiqueta)\n",
    "    y_test.append(etiqueta_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e591360-3f86-4746-a5a3-fea82514f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos totales\n",
      "-----------------\n",
      "Total: 78786\n",
      "-----------------\n",
      "Entrenamiento: 64466\n",
      "-----------------\n",
      "Prueba: 14320\n",
      "------------------------\n",
      "Positivos porcentajes\n",
      "-----------------\n",
      "Total: 0.28\n",
      "-----------------\n",
      "Entrenamiento: 0.29\n",
      "-----------------\n",
      "Prueba: 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Revisamos números de positivos en conjuntos\n",
    "print(\n",
    "    f\"Positivos totales\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Total: {sum(y_train) + sum(y_test)}\\n\"  # 277,524 tiles\n",
    "    \"-----------------\\n\"\n",
    "    f\"Entrenamiento: {sum(y_train) }\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Prueba: {sum(y_test) }\\n\"\n",
    "     \"------------------------\\n\"\n",
    "    f\"Positivos porcentajes\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Total: { round((sum(y_train) + sum(y_test))/ (len(y_train) + len(y_test)),2)  }\\n\"  # 277,524 tiles\n",
    "    \"-----------------\\n\"\n",
    "    f\"Entrenamiento: {round(sum(y_train)/ len(y_train),2) }\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Prueba: {round(sum(y_test)/ len(y_test),2) }\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad4ddc-7776-4967-aa20-fbda4c4dc086",
   "metadata": {},
   "source": [
    "## Preparamos las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9a2a76d-cb9e-4273-a41d-af319d80aaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 220389/220389 [30:46<00:00, 119.34it/s]\n"
     ]
    }
   ],
   "source": [
    "### Cargamos de imágenes de entrenamiento en una lista\n",
    "dataset_img_train = list()\n",
    "for img in tqdm(train_images_files):\n",
    "    image = Image.open(img)\n",
    "    image=image.resize((50,50))\n",
    "    numpydata = np.asarray(image)\n",
    "    dataset_img_train.append(numpydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b904e1d-7db3-4128-9591-91ca300d3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convertimos la lista en tensor\n",
    "dataset_img_train_array = np.asarray(dataset_img_train,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a623001-476b-4ac7-aaf3-91ac84c02f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Limpieza\n",
    "del train_images_files\n",
    "del etiqueta\n",
    "del etiqueta_num\n",
    "del dataset_img_train\n",
    "del numpydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f048e06-3c7a-4a70-a61f-2cba1a4062cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220389, 50, 50, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_img_train_array.shape ## Debe darnos un vector de tamaño (n,a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7312763-143b-4757-b6e6-323ff19bafaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.16 GiB for an array with shape (220389, 50, 50, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### Aplanamos los datos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset_img_train_array_plana \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_img_train_array\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.16 GiB for an array with shape (220389, 50, 50, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "### Aplanamos los datos\n",
    "dataset_img_train_array_plana = dataset_img_train_array.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ba1a2-1741-402a-bc4d-14bff7115a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_img_train_array_plana[0:3] ## Vemos las primeras entradas para verificar que estén aplanados los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf63e-38b9-4fa3-b7ff-6b8cd0b4e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_img_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a7e20-af82-4b97-9409-21b1870cab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revisamos que tengan la misma longitud para el conjunto de entrenamiento\n",
    "leny_train = len(y_train)\n",
    "lenset_train = len(dataset_img_train_array_plana)\n",
    "\n",
    "if leny_train == lenset_train:\n",
    "    print(\"El tamaño del vector de resultados y de las imágenes es el mismo en el conjunto de entrenamiento\")\n",
    "else:\n",
    "    print(\"El tamaño del vector de resultados y las imágenes no coincide en el conjunto de entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0552aa7-d034-4294-9279-08917ab2773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cargamos imágenes de prueba en una lista\n",
    "dataset_img_test = list()\n",
    "for img in tqdm(test_images_files):\n",
    "    image = Image.open(img)\n",
    "    image=image.resize((50,50))\n",
    "    numpydata = np.asarray(image)\n",
    "    numpydata_plana = numpydata.astype('float32') / 255\n",
    "    dataset_img_test.append(numpydata_plana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba325e-5dee-43f4-bc15-423b7f3c47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convertimos la lista en tensor\n",
    "dataset_img_test_array = np.asarray(dataset_img_test,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7ca55-d16b-45e9-857a-f5644750dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Limpieza\n",
    "del test_images_files\n",
    "del dataset_img_test\n",
    "del image\n",
    "del numpydata\n",
    "del numpydata_plana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e5b8d-b3fc-4d19-96b9-0c5c4e94f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_img_test_array.shape ## Debe darnos un vector de tamaño (n,a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491802a-e820-4323-9424-8b4908043578",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aplanamos los datos\n",
    "dataset_img_test_array_plana = dataset_img_test_array.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfa9b8-4946-45a2-99a4-60f0eee41d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_img_test_array_plana[0:3] ## Vemos las primeras entradas para verificar que estén aplanados los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de5070-acc3-462b-bcd7-577b7b6cf065",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revisamos que tengan la misma longitud para el conjunto de prueba\n",
    "leny_test = len(y_test)\n",
    "lenset_test = len(dataset_img_test_array_plana)\n",
    "\n",
    "if leny_test == lenset_test:\n",
    "    print(\"El tamaño del vector de resultados y de las imágenes es el mismo en el conjunto de prueba\")\n",
    "else:\n",
    "    print(\"El tamaño del vector de resultados y las imágenes no coincide en el conjunto de prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aeaf50-5440-4de5-8f7f-6c732bc56a1b",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2060b-39f0-48f2-85b6-fcf4e4286a03",
   "metadata": {},
   "source": [
    "## Conjunto de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93db94-200d-44a8-a136-3d2ddd36f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creamos conjuntos de validación y entrenamiento\n",
    "X_train, X_val, y_train, y_val = train_test_split(dataset_img_train_array_plana, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b39e1-ba6f-4a40-99e5-fb39b317a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Limpieza\n",
    "del dataset_img_train_array_plana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8718a-419b-479c-b095-a0d11aaedf53",
   "metadata": {},
   "source": [
    "## Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd590e-3aab-4480-a060-d1ff79c20cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Usamos modelo de la tarea 11\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same', input_shape=(50, 50, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "#model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8b290-818a-40f0-86c2-daeb80f21aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658cc7c-32ad-4d91-b13c-f819cff73603",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b3a47-b35c-4723-a7b4-e04eb12b98a3",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eba98e-f393-428e-b704-f9eb47cf17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entrenamos el modelo\n",
    "history = model.fit(X_train, y_train, batch_size=256, epochs=2, validation_data=(X_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e12d50-edad-4b59-9a63-c103859e5d59",
   "metadata": {},
   "source": [
    "## Gráficas de pérdida y precisión en el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafcbb1-8298-4f7e-9533-b64088dc4e0c",
   "metadata": {},
   "source": [
    "### Pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0063fb5-311f-478f-8ac1-26a29ff4ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hist, lr in zip(history, learning_rates):\n",
    "    loss = hist.history['loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'o-', label=lr)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f66c28-c9c2-4432-a77c-ddbf795379c5",
   "metadata": {},
   "source": [
    "### Precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0dc57-22bb-4734-993e-31f96cc0f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hist, lr in zip(history, learning_rates):\n",
    "    loss = hist.history['accuracy']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.semilogy(epochs, loss, 'o-', label=lr)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aabf76-f5ec-4569-8afa-2bccfd09ee43",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d649a55-bdb9-4e0b-8942-3b572813d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(dataset_img_test_array_plana)\n",
    "print(classification_report(y_test, np.argmax(pred,axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
