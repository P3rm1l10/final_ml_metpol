{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d1fc53-d94b-4499-817d-babbcb01e6b1",
   "metadata": {},
   "source": [
    "# Proyecto final Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6795c-ca8c-440d-bfeb-9b7607685711",
   "metadata": {},
   "source": [
    "Jorge Ruizvisfocri\n",
    "\n",
    "Emilio Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a7f13-99cc-46fe-a2cb-7f7bafe43117",
   "metadata": {},
   "source": [
    "# Descripción del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d469410-9a0e-4207-9278-5fe08489dffe",
   "metadata": {},
   "source": [
    "El siguiente proyecto busca clasificar imágenes de Carcinomas Ductales Invasivos (IDC en inglés) extraidas de muestras de pacientes con cancer de mama.\n",
    "\n",
    "La base de datos fue tomada de https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images\n",
    "\n",
    "Se propone utilizar una red neuronal convolucional para resolver el problema de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61434f9f-74a5-4dc9-ad23-b654c5949587",
   "metadata": {},
   "source": [
    "# Paqueterías de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c300dce-540b-42bd-a5b5-b8a14977d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paquetes de ciencias de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "import random\n",
    "\n",
    "## Paquetes de lectura de imagenes\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "## Paquetes de imágenes\n",
    "from skimage.io import imread\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ff47-7aa4-469d-9694-113bfeac7d62",
   "metadata": {},
   "source": [
    "# Datos de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74b69f5-85ae-4e92-8d98-1da5c635cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio de imágenes\n",
    "data_dir = Path(\"D://bases de datos//proyecto_ml_final//imagenes\") ## Directorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1eb82a-b910-47a6-a3d1-fcc431b716e2",
   "metadata": {},
   "source": [
    "Dado que todos los pacientes tienen muestras con tumores malignos y benignos, la aleatorización puede realizarse a nivel de imagen o a nivel de paciente.\n",
    "\n",
    "Bajo el supuesto de que podría existir correlación de algún tipo entre las imágenes pertenecientes a un mismo paciente, creemos que sería interesante aleatorizar a nivel de paciente, para introducirle a la red la información de personas que jamás ha visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ba3705-8755-4f5a-a53d-dfc487503281",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aleatorización a nivel de paciente\n",
    "#### Obtenemos la lista de pacientes\n",
    "px = [f for f in data_dir.iterdir() if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c287e8-2951-4e57-afce-25b89086617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtenemos el número equivalente al porcentaje deseado\n",
    "k = math.ceil( len(px) * 80 // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db14f1fd-217c-4779-a803-1bef7cc3b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hacemos la muestra de pacientes para entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d71ef82-8a3e-47b5-a8b3-6b45e7bda23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fijamos la semilla para tener reproductibilidad\n",
    "random.seed(4352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8cba47-2ad2-476a-b6d4-ed92d1e40714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = random.sample(px,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f427d55e-c414-4879-8be7-0176db51250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = set(px) - set(train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c6dd66-db7a-4b6d-9677-8c4f98bf6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cargamos files de entrenamiento\n",
    "train_images_files = []\n",
    "for p in train_id:\n",
    "    img_lst = list(p.rglob(\"*.png\"))\n",
    "    train_images_files.extend(img_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54b8fb0-5222-4e3a-85f4-b6629b21cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_files = []\n",
    "for p in test_id:\n",
    "    img_lst = list(p.rglob(\"*.png\"))\n",
    "    test_images_files.extend(img_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2694b47-372e-4572-a031-7ce7f90cc5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imagenes\n",
      "-----------------\n",
      "Total: 277524\n",
      "-----------------\n",
      "Entrenamiento: 220389\n",
      "-----------------\n",
      "Prueba: 57135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Revisamos que las direcciones de las imágenes estén bien cargadas\n",
    "print(\n",
    "    f\"Número de imagenes\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Total: {len(train_images_files) + len(test_images_files) }\\n\"  # 277,524 tiles\n",
    "    \"-----------------\\n\"\n",
    "    f\"Entrenamiento: {len(train_images_files) }\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Prueba: {len(test_images_files) }\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984508a-0785-4ac4-bdfb-32a840c306f9",
   "metadata": {},
   "source": [
    "## Preparamos las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0131a0b1-a854-49d1-9fe2-0327ddb64e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## y_entrenamiento\n",
    "### Preparamos las etiquetas\n",
    "y_train = []\n",
    "for name in train_images_files:\n",
    "    etiqueta = str(name)[-5]\n",
    "    etiqueta_num = int(etiqueta)\n",
    "    y_train.append(etiqueta_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62fb3993-3c47-479d-afe9-18b5f9fafd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "for name in test_images_files:\n",
    "    etiqueta = str(name)[-5]\n",
    "    etiqueta_num = int(etiqueta)\n",
    "    y_test.append(etiqueta_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c392d00-20e4-41f7-af6b-a96464103684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57135"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e591360-3f86-4746-a5a3-fea82514f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos\n",
      "-----------------\n",
      "Total: 78786.0\n",
      "-----------------\n",
      "Entrenamiento: 64466.0\n",
      "-----------------\n",
      "Prueba: 14320.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Revisamos números de positivos en conjuntos\n",
    "print(\n",
    "    f\"Positivos\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Total: {sum(y_train) + sum(y_test)}\\n\"  # 277,524 tiles\n",
    "    \"-----------------\\n\"\n",
    "    f\"Entrenamiento: {sum(y_train) }\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Prueba: {sum(y_test) }\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad4ddc-7776-4967-aa20-fbda4c4dc086",
   "metadata": {},
   "source": [
    "## Preparamos las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a2a76d-cb9e-4273-a41d-af319d80aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cargamos de imágenes de entrenamiento en una lista\n",
    "dataset_img_train = list()\n",
    "for img in tqdm(train_images_files):\n",
    "    image = Image.open(img)\n",
    "    image=image.resize((50,50))\n",
    "    numpydata = np.asarray(image)\n",
    "    dataset_img_train.append(numpydata)\n",
    "### Convertimos la lista en tensor\n",
    "dataset_img_train_array = np.asarray(dataset_img_train,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f048e06-3c7a-4a70-a61f-2cba1a4062cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_img_train_array.shape ## Debe darnos un vector de tamaño (n,a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e5a5ac7-850e-4f5d-849a-457001079fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aplanamos las imágenes\n",
    "data_set_img_train_plano = dataset_img_train_array.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "541a7e20-af82-4b97-9409-21b1870cab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vector de resultados y de las imágenes es el mismo en el conjunto de entrenamiento\n"
     ]
    }
   ],
   "source": [
    "## Revisamos que tengan la misma longitud para el conjunto de entrenamiento\n",
    "leny_train = len(y_train)\n",
    "lenset_train = len(data_set_img_train_plano)\n",
    "\n",
    "if leny_train == lenset_train:\n",
    "    print(\"El tamaño del vector de resultados y de las imágenes es el mismo en el conjunto de entrenamiento\")\n",
    "else:\n",
    "    print(\"El tamaño del vector de resultados y las imágenes no coincide en el conjunto de entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0552aa7-d034-4294-9279-08917ab2773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cargamos imágenes de prueba en una lista\n",
    "dataset_img_test = list()\n",
    "for img in tqdm(test_images_files):\n",
    "    image = Image.open(img)\n",
    "    image=image.resize((50,50))\n",
    "    numpydata = np.asarray(image)\n",
    "    dataset_img_test.append(numpydata)\n",
    "### Convertimos la lista en tensor\n",
    "dataset_img_test_array = np.asarray(dataset_img_test,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e5b8d-b3fc-4d19-96b9-0c5c4e94f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_img_test_array.shape ## Debe darnos un vector de tamaño (n,a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24d7e7c6-3eb5-4c99-ad31-03f1b324c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aplanamos las imágenes\n",
    "data_set_img_test_plano = dataset_img_test_array.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80de5070-acc3-462b-bcd7-577b7b6cf065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vector de resultados y de las imágenes es el mismo en el conjunto de prueba\n"
     ]
    }
   ],
   "source": [
    "## Revisamos que tengan la misma longitud para el conjunto de prueba\n",
    "leny_test = len(y_test)\n",
    "lenset_test = len(data_set_img_test_plano)\n",
    "\n",
    "if leny_test == lenset_test:\n",
    "    print(\"El tamaño del vector de resultados y de las imágenes es el mismo en el conjunto de prueba\")\n",
    "else:\n",
    "    print(\"El tamaño del vector de resultados y las imágenes no coincide en el conjunto de prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aeaf50-5440-4de5-8f7f-6c732bc56a1b",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb93db94-200d-44a8-a136-3d2ddd36f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creamos conjuntos de validación y entrenamiento\n",
    "X_val = np.asarray(data_set_img_test_plano[47135:],dtype=object)\n",
    "y_val = np.asarray(y_train[47135:],dtype=object)\n",
    "X_train_2 = np.asarray(data_set_img_test_plano[:47135],dtype=object)\n",
    "y_train_2 = np.asarray(y_train[:47135],dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "11dd590e-3aab-4480-a060-d1ff79c20cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Usamos modelo de la tarea 11\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same', input_shape=(50, 32, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "#model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17b8b290-818a-40f0-86c2-daeb80f21aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f658cc7c-32ad-4d91-b13c-f819cff73603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 50, 50, 32)        896       \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 50, 50, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 25, 25, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 25, 25, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 12, 12, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 9218      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,642\n",
      "Trainable params: 148,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0eba98e-f393-428e-b704-f9eb47cf17e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Entrenamos el modelo\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "## Entrenamos el modelo\n",
    "history = model.fit(X_train_2, y_train_2, batch_size=256, epochs=2, validation_data=(X_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "39681033-cdb1-45b2-a004-a6130059aad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea3d405e-32a0-4e10-9fad-6fe44255ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_6756\\3453173665.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(X_train_2)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3d829f5-0e30-4c81-8777-e38884e43390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6901961 , 0.5411765 , 0.8666667 ],\n",
       "        [0.7019608 , 0.5686275 , 0.7411765 ],\n",
       "        [0.8117647 , 0.7294118 , 0.8901961 ],\n",
       "        ...,\n",
       "        [0.6627451 , 0.4862745 , 0.88235295],\n",
       "        [0.6509804 , 0.49803922, 0.8784314 ],\n",
       "        [0.58431375, 0.39215687, 0.85490197]],\n",
       "\n",
       "       [[0.7647059 , 0.654902  , 0.8862745 ],\n",
       "        [0.6392157 , 0.4509804 , 0.69803923],\n",
       "        [0.827451  , 0.7372549 , 0.9137255 ],\n",
       "        ...,\n",
       "        [0.5882353 , 0.4       , 0.8509804 ],\n",
       "        [0.6156863 , 0.44313726, 0.81960785],\n",
       "        [0.5803922 , 0.3882353 , 0.81960785]],\n",
       "\n",
       "       [[0.7490196 , 0.627451  , 0.8745098 ],\n",
       "        [0.5647059 , 0.3882353 , 0.7607843 ],\n",
       "        [0.62352943, 0.4627451 , 0.81960785],\n",
       "        ...,\n",
       "        [0.59607846, 0.39215687, 0.85490197],\n",
       "        [0.6666667 , 0.5019608 , 0.8745098 ],\n",
       "        [0.5137255 , 0.30588236, 0.7607843 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.5529412 , 0.34117648, 0.5882353 ],\n",
       "        [0.81960785, 0.7882353 , 0.8666667 ],\n",
       "        [0.5921569 , 0.40784314, 0.6392157 ],\n",
       "        ...,\n",
       "        [0.7294118 , 0.6       , 0.80784315],\n",
       "        [0.78039217, 0.67058825, 0.8235294 ],\n",
       "        [0.6901961 , 0.5411765 , 0.827451  ]],\n",
       "\n",
       "       [[0.54509807, 0.3529412 , 0.6117647 ],\n",
       "        [0.5568628 , 0.3647059 , 0.5764706 ],\n",
       "        [0.57254905, 0.3647059 , 0.6392157 ],\n",
       "        ...,\n",
       "        [0.89411765, 0.84705883, 0.92156863],\n",
       "        [0.6431373 , 0.4392157 , 0.80784315],\n",
       "        [0.6666667 , 0.49411765, 0.8784314 ]],\n",
       "\n",
       "       [[0.64705884, 0.4627451 , 0.73333335],\n",
       "        [0.6901961 , 0.5176471 , 0.7411765 ],\n",
       "        [0.63529414, 0.4392157 , 0.70980394],\n",
       "        ...,\n",
       "        [0.6901961 , 0.52156866, 0.83137256],\n",
       "        [0.654902  , 0.49019608, 0.88235295],\n",
       "        [0.6901961 , 0.52156866, 0.8980392 ]]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176dd65d-5e77-48c4-8814-16bd76c12da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
